{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ¿Qué es un árbol de decisión?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Un árbol de decisión es un mapa de los posibles resultados de una serie de decisiones relacionadas. Permite que un individuo o una organización comparen posibles acciones entre sí según sus costos, probabilidades y beneficios. Se pueden usar para dirigir un intercambio de ideas informal o trazar un algoritmo que anticipe matemáticamente la mejor opción.\n",
    "Un árbol de decisión, por lo general, comienza con un único nodo y luego se ramifica en resultados posibles. Cada uno de esos resultados crea nodos adicionales, que se ramifican en otras posibilidades. Esto le da una forma similar a la de un árbol.\n",
    "Hay tres tipos diferentes de nodos: nodos de probabilidad, nodos de decisión y nodos terminales. Un nodo de probabilidad, representado con un círculo, muestra las probabilidades de ciertos resultados. Un nodo de decisión, representado con un cuadrado, muestra una decisión que se tomará, y un nodo terminal muestra el resultado definitivo de una ruta de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "   ## Reglas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "En los árboles de decisión se tiene que cumplir una serie de reglas.\n",
    "    1.\tAl comienzo del juego se da un nodo inicial que no es apuntado por ninguna flecha, es el único del juego con esta característica.\n",
    "    2.\tEl resto de nodos del juego son apuntados por una única flecha.\n",
    "    3.\tDe esto se deduce que hay un único camino para llegar del nodo inicial a cada uno de los nodos del juego. No hay varias formas de llegar a la misma solución final, las decisiones son excluyentes.\n",
    "\n",
    "En los árboles de decisiones las decisiones que se eligen son lineales, a medida que vas seleccionando entre varias opciones se van cerrando otras, lo que implica normalmente que no hay marcha atrás. En general se podría decir que las normas siguen una forma condicional: Opción 1->opción 2->opción 3->Resultado Final X Estas reglas suelen ir implícitas en el conjunto de datos a raíz del cual se construye el árbol de decisión.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ventajas y desventajas "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Los árboles de decisión siguen siendo populares por razones como las siguientes:\n",
    "•\tSon muy fáciles de entender\n",
    "•\tPueden ser útiles con o sin datos fehacientes, y cualquier dato requiere una preparación mínima\n",
    "•\tSe pueden agregar nuevas opciones a los árboles existentes\n",
    "•\tSu valor al seleccionar la mejor de numerosas opciones\n",
    "•\tSe combinan fácilmente con otras herramientas de toma de decisiones\n",
    "Sin embargo, los árboles de decisión pueden volverse excesivamente complejos. En esos casos, un diagrama de influencia más compacto puede ser una buena alternativa. Los diagramas de influencia se enfocan en los objetivos, las entradas y las decisiones fundamentales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hombre']\n"
     ]
    }
   ],
   "source": [
    "#Se importa la librería sklearn el módulo tre\n",
    "from sklearn import tree\n",
    "\n",
    "#Se crea la instancia del árbol de decisión.\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "\n",
    "#[altura, peso, talla de zapato]\n",
    "X = [[181, 80, 44], [177, 70, 43], [160, 60, 38], [154, 54, 37], [166, 65, 40],\n",
    "     [190, 90, 47], [175, 64, 39],\n",
    "     [177, 70, 40], [159, 55, 37], [171, 75, 42], [181, 85, 43]]\n",
    "\n",
    "#La salida donde se dice si es hombre o mujer\n",
    "\n",
    "Y = ['hombre', 'hombre', 'mujer', 'mujer', 'hombre', 'hombre', 'mujer', 'mujer',\n",
    "     'mujer', 'hombre', 'hombre']\n",
    "\n",
    "#Se le pasa los datos  X y Y\n",
    "clf = clf.fit(X, Y)\n",
    "\n",
    "#Se definen los datos 1 y 2\n",
    "\n",
    "dato1 = [190, 70, 43]\n",
    "dato2 = [185, 62, 37]\n",
    "prediction = clf.predict([dato1])\n",
    "\n",
    "#Se muestra el resultado de la predicción de dato1\n",
    "\n",
    "print(prediction)\n",
    "prediction = clf.predict([dato2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Es una combinación de árboles predictores tal que cada árbol depende de los valores de un vector aleatorio probado independientemente y con la misma distribución para cada uno de estos. Es una modificación sustancial de bagging que construye una larga colección de árboles no correlacionados y luego los promedia.\n",
    "\n",
    "La idea esencial del bagging es promediar muchos modelos ruidosos pero aproximadamente imparciales, y por tanto reducir la variación. Los árboles son los candidatos ideales para el bagging, dado que ellos pueden registrar estructuras de interacción compleja en los datos, y si crecen suficientemente profundo, tienen relativamente baja parcialidad. Producto de que los árboles son notoriamente ruidosos, ellos se benefician enormemente al promediar.\n",
    "Cada árbol es construido usando el siguiente algoritmo:\n",
    "    1.\tSea N el número de casos de prueba, M es el número de variables en el clasificador.\n",
    "    2.\tSea m el número de variables de entrada a ser usado para determinar la decisión en un nodo dado; m debe ser mucho menor que M\n",
    "    3.\tElegir un conjunto de entrenamiento para este árbol y usar el resto de los casos de prueba para estimar el error.\n",
    "    4.\tPara cada nodo del árbol, elegir aleatoriamente m variables en las cuales basar la decisión. Calcular la mejor partición del conjunto de entrenamiento a partir de las m variables.\n",
    "    \n",
    "Para la predicción un nuevo caso es empujado hacia abajo por el árbol. Luego se le asigna la etiqueta del nodo terminal donde termina. Este proceso es iterado por todos los árboles en el ensamblado, y la etiqueta que obtenga la mayor cantidad de incidencias es reportada como la predicción.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Características (o rasgos) y Ventajas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Las ventajas del random forests son:\n",
    "    •\tEs uno de los algoritmos de aprendizaje más certeros que hay disponible. Para un set de datos lo suficientemente grande produce un clasificador muy certero.7\n",
    "    •\tCorre eficientemente en bases de datos grandes.\n",
    "    •\tPuede manejar cientos de variables de entrada sin excluir ninguna.\n",
    "    •\tDa estimados de qué variables son importantes en la clasificación.\n",
    "    •\tTiene un método eficaz para estimar datos perdidos y mantener la exactitud cuando una gran proporción de los datos está perdida.\n",
    "    •\tComputa los prototipos que dan información sobre la relación entre las variables y la clasificación.\n",
    "    •\tComputa las proximidades entre los pares de casos que pueden usarse en los grupos, localizando valores atípicos, o (ascendiendo) dando vistas interesantes de los datos.\n",
    "    •\tOfrece un método experimental para detectar las interacciones de las variables.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Desventajas"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    •\tSe ha observado que Random forests sobreajusta en ciertos grupos de datos con tareas de clasificación/regresión ruidosas.\n",
    "    •\tA diferencia de los árboles de decisión, la clasificación hecha por random forests es difícil de interpretar por el hombre.\n",
    "    •\tPara los datos que incluyen variables categóricas con diferente número de niveles, el random forests se parcializa a favor de esos atributos con más niveles. Por consiguiente, la posición que marca la variable no es fiable para este tipo de datos. Métodos como las permutaciones parciales se han usado para resolver el problema1011\n",
    "    •\tSi los datos contienen grupos de atributos correlacionados con similar relevancia para el rendimiento, entonces los grupos más pequeños están favorecidos sobre los grupos más grandes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ¿Cómo se construye un random forest?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "¿Cómo se construye un random forest?\n",
    "La fase de aprendizaje consiste en crear muchos árboles de decisión independientes, construyéndolos a partir de datos de entrada ligeramente distintos. Se altera, por tanto, el conjunto inicial de partida, haciendo lo siguiente:\n",
    "– Se selecciona aleatoriamente con reemplazamiento un porcentaje de datos de la muestra total.\n",
    "Es habitual incluir un segundo nivel aleatoriedad, esta vez afectando los atributos:\n",
    "– En cada nodo, al seleccionar la partición óptima, tenemos en cuenta sólo una porción de los atributos, elegidos al azar en cada ocasión.\n",
    "Una vez que tenemos muchos árboles, 1000 por ejemplo, la fase de clasificación se lleva a cabo de la siguiente forma:\n",
    "– Cada árbol se evalúa de forma independiente y la predicción del bosque será la media de los 1000 árboles. La proporción de árboles que toman una misma respuesta se interpreta como la probabilidad de la misma.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Referencias"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "https://quantdare.com/random-forest-vs-simple-tree/\n",
    "http://www.uam.es/personal_pdi/ciencias/jspinill/CFCUAM2014/RF_BRT-CFCUAM2014.html\n",
    "http://randomforest2013.blogspot.com/2013/05/randomforest-definicion-random-forests.html\n",
    "http://apuntes-r.blogspot.com/2014/11/ejemplo-de-random-forest.html\n",
    "https://rpubs.com/Joaquin_AR/255596"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
